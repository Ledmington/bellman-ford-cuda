\documentclass[a4paper]{article}

\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{csquotes}

\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}

\usepackage[
linesnumbered,
ruled,
titlenotnumbered,
italiano
]{algorithm2e}

\addbibresource{tesi.bib}

\renewcommand{\arraystretch}{1.2}

\begin{document}
	
	\newgeometry{margin=0.8in}
	\begin{titlepage}
		\begin{center}
			
			\large
			\textbf{ALMA MATER STUDIORUM -- UNIVERSITÀ DI BOLOGNA \\ CAMPUS DI CESENA} \\
			\noindent\hrulefill
			\vspace{0.4cm}
			
			\Large
			Scuola di Ingegneria e Architettura \\
			Corso di Laurea Triennale in Ingegneria e Scienze Informatiche
			
			\Huge
			\vspace{4cm}
			\textbf{
				Implementazione CUDA \\
				dell'algoritmo di Bellman-Ford
			}
			
			\large
			\vspace{1cm}
			Tesi di laurea in \\
			\textsc{High-Performance Computing}
			
			\vspace{5.5cm}
			\begin{minipage}[t]{0.64\textwidth}
				\begin{flushleft}
					\textit{Relatore} \\ 
					\textbf{Prof.} \textbf{Moreno Marzolla}
				\end{flushleft}
			\end{minipage}
			\begin{minipage}[t]{0.34\textwidth}
				\begin{flushright}
					\textit{Candidato} \\ 
					\textbf{Filippo Barbari}
				\end{flushright}
			\end{minipage}\\
			
			\vfill
			\noindent\hrulefill
			\vspace{0.3cm}
			\Large
			
			Seconda Sessione di Laurea \\
			Anno Accademico 2020-2021
		\end{center}
	\end{titlepage}
	\restoregeometry
	\newpage
	
	\tableofcontents
	\newpage
	
	\listoffigures
	\newpage
	
	\listoftables
	\newpage
	
	\listofalgorithms
	\newpage
	
	\section{Introduzione}
	L'obiettivo di questa tesi consiste nel trovare l'implementazione più efficiente, in linguaggio CUDA, dell'algoritmo di Bellman-Ford per il Single-Source Shortest Path problem (SSSP). Allo scopo di svolgere un'analisi e una ricerca quanto più complete possibili, in questa tesi misuriamo le prestazioni di varie implementazioni utilizzando vari tipi di test su 3 macchine diverse. L'algoritmo di Bellman-Ford è stato uno dei primi ad essere proposto come soluzione del SSSP problem e, nonostante le varie ottimizzazioni che ha ricevuto, oggigiorno non è più molto utilizzato a causa di altri algoritmi più recenti ed efficienti. Tuttavia, è probabilmente l'algoritmo che meglio si presta alla parallelizzazione massiva offerta dal linguaggio CUDA.
	
	Il capitolo \ref{section:storia} è un'introduzione al calcolo parallelo: alcuni cenni storici, l'evoluzione dell'hardware fino ai giorni nostri e le strade verso cui potrebbe dirigersi. Nel capitolo \ref{section:analisi} si analizza il problema del Single-Source Shortest Path e poi l'algoritmo proposto da Bellman e Ford. Nella stessa sezione, inoltre, si mostra la sua correttezza e i suoi pregi e difetti. Nella sezione \ref{section:design}, partendo dall'analisi del capitolo precedente, si enumerano un insieme di possibili parallelizzazioni dell'algoritmo spiegando quale potrebbe essere la migliore e perchè. Nel capitolo \ref{section:impl} si discuterà dei dettagli implementativi non trascurabili che saranno presenti nel repository. Infine, nel capitolo \ref{section:perf} si mostreranno i risultati delle misure più comuni che è possibile misurare su GPU (speedup rispetto alla versione seriale e throughput) e si discuteranno.
	
	\section{Calcolo parallelo}
	\label{section:storia}
	Qui si parla della storia del calcolo parallelo, perchè è necessario, perchè si sta sviluppando così tanto e perchè le GPU sono diventate cosi importanti.
	
	Negli anni '60, Gordon Moore, futuro fondatore e direttore esecutivo di Intel, ipotizzò che il numero di componenti elettronici (transistor) contenuti all'interno di un chip sarebbe raddoppiato ogni 18 mesi circa. Tale previsione si rivelò corretta per tutti gli anni '70 e i tempi si allungarono a partire dagli anni '80. Intorno agli anni 2000 si è giunti all'epilogo di questa crescita, infatti era evidente già un decennio prima che si sarebbero raggiunti presto i limiti fisici all'interno di un processore. Per questo motivo, la potenza di calcolo dei processori single-core è cresciuta sempre più lentamente fino ad arrestarsi e si è dovuti ricorrere alle architetture parallele. Raddoppiare il numero di cores fisici all'interno di un chip può, in certi casi, raddoppiare le prestazioni e ridurre allo stesso tempo il consumo di energia ma richiede uno sforzo maggiore da parte del programmatore per lo sviluppo di codice parallelo. A causa di questa difficoltà e anche a causa della scarsità di programmatori qualificati, il software parallelo è molto meno presente dell'hardware parallelo.
	
	Intorno al 2007, sono apparse sul mercato le prime General-Purpose Graphic Processing Unit (GPGPU), ovvero le prime schede grafiche programmabili in maniera generica. Queste hanno aperto una nuova era del calcolo parallelo, permettendo ai programmatori di sfruttare l'elevatissimo numero di cores fisici per computazioni estremamente parallelizzabili. La differenza principale, a livello hardware, tra una CPU multi-core e una GPGPU consiste nel numero di core quasi "centuplicato" a parità di prezzo. Questa abbondanza di risorse di calcolo ha cambiato anche il modo di strutturare i programmi paralleli. Le GPGPU hanno determinato un cambiamento così importante in ambito HPC che si può notare come, all'incirca dal 2010 in poi, tutti i nuovi super-computer costruiti possiedono un elevato numero di GPGPU affiancate alle CPU.
	
	\subsection{GPU e CUDA}
	Qui si parla di cos'è CUDA e come funziona. Si parla anche della struttura della memoria di una GPU CUDA e di come funziona lo scheduling. Bisogna citare OpenCL?
	
	CUDA (acronimo di Compute Unified Device Architecture) è un'architettura hardware realizzata da NVIDIA, la cui prima versione è stata pubblicata nel 2007. Tramite CUDA è possibile scrivere programmi che saranno eseguiti su di una scheda grafica NVIDIA e su una CPU. CUDA basa tutto il suo funzionamento su alcuni concetti molto semplici: la CPU su cui viene lanciato il programma è chiamata \textit{host} e può interagire con una o più schede grafiche NVIDIA, chiamate \textit{device}. L'host può invocare alcune funzioni specifiche che vengono eseguite dal device, per questo motivo, ogni invocazione a queste funzioni è di fatto asincrona, ovvero il controllo viene restituito all'host subito, senza attendere il completamento della funzione invocata. Tuttavia, esistono alcune funzioni che permettono di sincronizzare esplicitamente l'host con un device. Le funzioni più importanti sono i \textit{kernel}, ovvero le funzioni che possono essere scritte dal programmatore per effettuare qualunque tipo di calcolo. I kernel, come le altre funzioni CUDA, sono invocati in maniera asincrona e, a differenza delle altre, richiedono due parametri obbligatori, ovvero il numero di blocchi e il numero di thread per blocco da utilizzare per eseguire quel kernel.
	
	CUDA raggruppa tutti i thread, ovvero le singole unità di esecuzione logiche, in blocchi (in genere da 1024) che a loro volta sono raggruppati in una griglia. Ogni device ha sempre e solo una griglia in esecuzione e ogni griglia esegue sempre e solo un singolo kernel. I blocchi all'interno della grigla sono indicizzabili con 1, 2 o 3 dimensioni e lo stesso vale per i thread all'interno di un blocco. La memoria di un device CUDA è suddivisa in vari livelli:
	\begin{itemize}
		\item la \textbf{memoria globale} è la memoria più voluminosa all'interno di un device (alcuni GB) ed è accessibile da tutti i thread di tutti i blocchi
		\item la \textbf{memoria texture} è molto più piccola di quella globale (alcuni MB) ed è ottimizzata per contenere dati di texture, è accessibile da tutti i thread di tutti i blocchi
		\item la \textbf{memoria delle costanti} è una memoria molto piccola (alcuni KB) e di sola lettura (da controllare), è accessibile da tutti i thread di tutti i blocchi
		\item la \textbf{memoria shared} è una piccola memoria (alcuni MB) molto più veloce della memoria globale che però è accessibile solamente da thread all'interno dello stesso blocco
		\item la \textbf{memoria locale} è la memoria "privata" di ogni thread e per questo motivo è accessibile solamente da un singolo thread
	\end{itemize}
	Aggiungere immagine struttura memoria in architettura CUDA
	
	Lo scheduling in una scheda CUDA funziona in modo particolare. Innanzitutto, va precisato che, nonostante il numero di cores fisici (Streaming Multiprocessors) vari in quasi ogni scheda, non è possibile scegliere quali e quanti utilizzare per l'esecuzione di una funzione kernel. Quando invochiamo un kernel, infatti, i thread che abbiamo richiesto vengono creati in memoria ma poi è l'hardware che sceglie la schedulazione migliore di questi thread sui SM a disposizione. Per questo motivo, si utilizza il concetto di \textit{thread warp}: un gruppo di thread (genericamente massimo 32) appartenenti allo stesso blocco che devono eseguire la stessa porzione di codice vengono schedulati tutti insieme sullo stesso SM ed eseguiti contemporaneamente. In caso di una biforcazione del flusso di controllo (ad esempio a causa di un \texttt{if-else}), viene verificata la condizione per tutti i thread, dopodichè vengono eseguiti tutti insieme solamente quelli che hanno verificato la condizione, poi vengono eseguiti tutti insieme tutti gli altri, poi l'esecuzione procede normalmente.
	
	\section{Teoria}
	\subsection{Grafi}
	Un grafo $G$ è genericamente indicato come una coppia ordinata di insiemi $(V,E)$ in cui $V$ rappresenta l'insieme dei nodi e $E$ l'insieme degli archi. Ogni elemento di $E$ è una coppia di elementi di $V$ da cui segue che $E \subseteq V \times V$. Una prima classificazione dei grafi consiste nel peso degli archi: un grafo si dice \textbf{non pesato} se i suoi archi hanno lo stesso peso (o, per semplicità, non hanno peso), altrimenti si definisce \textbf{pesato}. Un'ulteriore classificazione dei grafi consiste nella simmetria degli archi: se ogni arco è simmetrico si dice che il grafo è \textbf{non orientato} (o indiretto), altrimenti si dice \textbf{orientato} (o diretto). Più formalmente, se $w:E \rightarrow \Re$ è una funzione che restituisce il peso di un arco, allora un arco $(u,v)$ è simmetrico se e solo se:
	\begin{itemize}
		\item $(u,v) \in E$ e $(v,u) \in E$
		\item $w(u,v) = w(v,u)$
	\end{itemize}
	Se il grafo non è orientato, in genere si considera $E$ semplicemente come un insieme di coppie non ordinate di nodi in cui $(u,v)$ e $(v,u)$ rappresentano lo stesso arco.
	Inoltre, è possibile considerare anche i cosiddetti "cappi" o "auto-anelli", ovvero archi della forma $(u,u)$. Se un grafo non possiede auto-anelli, allora si definisce \textbf{grafo semplice}. In un grafo semplice il numero massimo possibile di archi è pari a $|V|\cdot (|V|-1)$, invece in un grafo non semplice corrisponde a $|V|^2$. Se il grafo è non orientato, il numero massimo di archi va diviso per 2. Se un grafo possiede esattamente il numero massimo di archi si dice \textbf{completo}.
	Una metrica molto utilizzata è la \textbf{densità} di un grafo, che si calcola come il rapporto tra il numero di archi e il numero massimo possibile di archi. Se un grafo ha "pochi" archi, dove per pochi si intende che il numero di archi è comparabile o inferiore al numero di nodi, si dice \textbf{sparso}, altrimenti si dice \textbf{denso}. Gli archi diretti $(u,v)$ si dicono uscenti per il nodo $u$ ed entranti per il nodo $v$. Se un nodo non ha archi nè entranti nè uscenti, si dice \textbf{isolato}.
	
	Un grafo si dice planare se è possibile rappesentarlo su di un piano senza che nessuno dei suoi archi si intersechi con altri archi. Più formalmente, si utilizza la \textbf{caratteristica di Eulero}, ovvero se un grafo è planare allora vale la relazione $v-e+f=2$ in cui $v$ è il numero di nodi, $e$ è il numero di archi e $f$ è il numero di facce.
	
	\subsection{Cammini minimi e problema SSSP}
	\label{section:sssp}
	In un grafo, un qualunque percorso che congiunge due nodi è detto \textbf{cammino} ed è rappresentabile come una lista ordinata di nodi. \'E importante notare la possibilità che uno stesso nodo sia presente più volte all'interno dello stesso cammino, quando ciò accade si parla di \textbf{cicli} (che approfondiremo di seguito). Il primo e l'ultimo nodo di questa lista ordinata sono chiamati rispettivamente \textbf{nodo sorgente} e \textbf{nodo destinazione}. In un grafo non pesato, il costo (o peso) di un cammino è pari al numero di nodi nella lista mentre in un grafo pesato è pari alla somma dei pesi degli archi coinvolti. Un cammino è detto \textbf{minimo} se non esiste un altro cammino nello stesso grafo, con uguali nodi sorgente e destinazione, con costo strettamente minore. Nello stesso grafo, a parità di nodi sorgente e destinazione, possono esistere numerosi cammini minimi diversi tra loro.
	
	Un ciclo consiste in un sottoinsieme $C$ di nodi di un grafo $G=(V,E)$ tali che $\forall u,v \in C, (u,v) \in E$. Un albero, ad esempio, è un tipo particolare di grafo che non presenta cicli. Il costo (o peso) di un ciclo è pari alla somma dei pesi degli archi tra i nodi da cui è composto (nei grafi non pesati, invece, è pari al numero di nodi). \'E possibile che in un grafo pesato siano presenti archi di costo negativo: sebbene siano poche le situazioni reali modellabili in questo modo, è molto importante determinare la presenza di cicli di costo negativo. Se in un grafo è presente un ciclo di costo negativo, allora ogni cammino minimo (in cui è possibile raggiungere tale ciclo dal nodo sorgente) avrà costo $-\infty$. Ai fini computazionali, non riuscire ad individuare un ciclo di costo negativo può determinare lo spreco di molte risorse di calcolo e di tempo eseguendo un algoritmo che produce un risultato inutile. Per questo motivo, in genere si utilizzano solamente grafi con pesi strettamente positivi.
	
	Un cammino minimo rispetta la cosiddetta \textbf{proprietà della sottostruttura ottima} (aggiungere immagine d'esempio), ovvero separando un cammino minimo in qualunque suo punto si ottengono due cammini minimi. Più formalmente, se abbiamo un cammino minimo $C$ dal nodo $u$ al nodo $v$ in cui è incluso anche il nodo $t$, allora sappiamo che tutti i nodi in $C$ da $u$ a $t$ costituiscono un cammino minimo e anche tutti i nodi in $C$ tra $t$ e $v$. (ci vuole una dimostrazione?)
	
	Una proprietà importante di un grafo è la \textbf{disuguaglianza triangolare} (aggiungere immagine). In un triangolo non degenere (ovvero i cui tre vertici non sono allineati) la somma delle lunghezze di due lati è sempre strettamente maggiore della lunghezza del terzo lato. Posta in maniera più semplice, dati tre punti nello spazio $x$, $y$ e $z$, la distanza tra $x$ e $y$ è minore o uguale alla somma delle distanze tra $x$ e $z$ e tra $z$ e $y$. Questa proprietà risulterà molto importante nella spiegazione della procedura RELAX\ref{alg:relax}.
	
	Dei tanti problemi che si possono applicare ad un grafo, il più studiato (su cui si concentra questa tesi) è senz'altro il Single-Source Shortest Path (abbreviato SSSP). Esso consiste, dato un grafo $G=(V,E)$ e un nodo sorgente $s$, nel trovare tutti i cammini minimi in $G$ che partono da $s$ e terminano in ogni altro nodo. Letteralmente significa "cammino minimo a singola sorgente", per differenziarlo dalla sua generalizzazione, l'All-Pairs Shortest Path (APSP) che invece chiede di trovare il cammino minimo per ogni coppia di vertici. Il risultato del SSSP consiste in un array di distanze dal nodo sorgente e in un array di predecessori tramite cui è possibile costruire l'albero dei cammini minimi.
	
	\section{Metriche di valutazione}
	\label{section:metriche}
	\subsection{Speedup}
	Perchè è molto importante e perchè non ha senso misurarlo sulle applicazioni CUDA? Noi lo misureremo lo stesso ma solo rispetto alla versione seriale su CPU.
	
	Lo speedup è la principale metrica di misurazione di un programma parallelo. Essa, infatti, rappresenta l'accelerazione che un dato programma ha ricevuto tramite la parallelizzazione. In particolare, lo speedup di un programma parallelizzato su $p$ cores si calcola come:
	\begin{equation}
		S(p) = \frac{T_s}{T(p)}
		\label{eq:speedup}
	\end{equation}
	in cui $T_s$ rappresenta il tempo di esecuzione del programma seriale (la "versione originale") e $T(p)$ rappresenta il tempo di esecuzione del programma parallelo utilizzando $p$ cores.
	
	In generale, $S(p) < p$. Idealmente, si vorrebbe mantenere uno \textbf{speedup lineare} nel proprio programma, ovvero $S(p) = p$ indipendentemente dal numero di processori utilizzati. Anche se può sembrare controintuitivo, raddoppiando il numero di processori utilizzati non si ottiene quasi mai un dimezzamento perfetto del tempo di esecuzione; ciò è dovuto all'aumentare dei tempi per la sincronizzazione e la comunicazione, oltre all'\textit{overhead} per l'allocazione della memoria necessaria. Nonostante questo, esistono alcuni rari casi in cui è possibile misurare nella realtà uno \textbf{speedup superlineare}, ovvero $S(p) > p$. In questi casi, la misurazione indica che raddoppiando il numero di processori il tempo di esecuzione si è più che dimezzato. Questo fenomeno, più unico che raro, è dovuto nella maggior parte dei casi alle elevate velocità e dimensioni delle cache integrate all'interno dei processori utilizzati.
	
	Come è possibile notare dalla formula \ref{eq:speedup}, lo speedup di un programma parallelo si osserva misurando il tempo di esecuzione man mano che si aumenta il numero di processori utilizzati. In questo modo, si osserva "quanto peggiora lo speedup" aumentando il numero di processori utilizzati. Tuttavia, questo non è possibile con un programma CUDA perchè il programmatore non ha potere decisionale sul numero di cores fisici da utilizzare nè sulla schedulazione dei thread logici: l'unico aspetto della computazione che può controllare è la dimensione della \textit{grid} (intesa come numero di blocchi e numero di thread per ciascun blocco). Per questo motivo, in genere non ha senso misurare lo speedup di un'applicazione parallela su CUDA semplicemente aumentando il numero di thread. Tuttavia, in questa tesi misuriamo un singolo valore di accelerazione per ogni algoritmo per meglio confrontare le varie versioni.
	
	\subsection{Throughput}
	Il throughput (o produttività) è una metrica molto più rappresentativa delle prestazioni di un'applicazione CUDA per i motivi citati nella sezione precedente. Esso consiste nel numero di "elementi" processati in un secondo, oppure di "unità di lavoro" eseguite nello stesso tempo. Cosa si intenda per unità di lavoro, però, dipende dal problema, dal tipo di dati in input e dall'implementazione di un certo algoritmo. Ad esempio, per un'applicazione che applica filtri di colore ad un'immagine il throughput può essere misurati come numero di pixel modificati al secondo. Un altro esempio può essere un'algoritmo che cerca la mossa migliore in una partita di scacchi per il quale la produttività si può misurare come il numero di stati della scacchiera analizzati ogni secondo. Nel nostro caso, il throughput è misurato come il numero di rilassamenti di un arco effettuati ogni secondo.
	
	Ogni algoritmo ha un limite massimo di throughput che è possibile raggiungere solamente aumentando le dimensioni dei dati di input. In altre parole, è necessario fornire sufficienti dati all'algoritmo per farlo "andare a regime" (da cambiare assolutamente).
	
	\subsection{Intensità aritmetica}
	L'intensità aritmetica (o intensità operativa) è una metrica per confrontare due implementazioni diverse dello stesso algoritmo. Si calcola come il rapporto tra il numero di operazioni necessarie a terminare una certa unità di lavoro e il numero di byte necessari per la memoria. Il valore ottenuto rappresenta "quanto è computation-based o memory-based" una certa implementazione di un algoritmo. Aggiungere esempio?
	
	Noi la utilizzeremo per spiegare meglio i valori ottenuti dalle misurazioni di speedup e throughput delle varie versioni dell'algoritmo.
	
	\section{L'algoritmo di Bellman Ford}
	\label{section:analisi}
	Proposto per primo da Alfonso Shimbel nel 1955 \cite{Shimbel1955}, ma è chiamato Bellman-Ford perchè Richard Bellman l'ha pubblicato nel 1958 \cite{Bellman1958} e Lester Ford Jr. l'ha pubblicato nel 1956 \cite{Ford1956}. Edward F. Moore ha pubblicato una sua variante nel 1959 \cite{Moore1959} (per questo a volte viene chiamato Bellman-Ford-Moore). La variante di Moore è nota come Shortest Path Faster Algorithm. Yen nel 1970 \cite{Yen1970} ha proposto una nuova variante per ridurre il numero di passi di rilassamento. Bannister e Eppstein nel 2012 \cite{Bannister2012} hanno proposto una nuova variante per ridurre ulteriormente i passi di rilassamento.
	
	La versione che riportiamo di seguito e che andremo ad implementare è quella originale proposta prima da Lester Ford Jr. e poi da Richard Bellman.
	
	\begin{algorithm}[H]
		\label{alg:bf}
		\SetAlgorithmName{Bellman-Ford}{}{}
		\KwIn{G, w, s}
		\KwResult{FALSE se il grafo G contiene cicli di costo negativo, TRUE altrimenti}
		Initialize-Single-Source(G,s)\;
		\For{i=1 \textbf{to} |G.V| - 1}{
			\ForEach{edge (u,v) $\in$ G.E}{
				Relax(u,v,w)\;
			}
		}
		\ForEach{edge (u,v) $\in$ G.E}{
			\If{v.d > u.d + w(u,v)}{
				\Return FALSE\;
			}
		}
		\Return TRUE\;
		\caption{L'algoritmo di Bellman-Ford}
	\end{algorithm}

	Come è possibile notare dallo pseudocodice, il corpo effettivo dell'algoritmo consiste nelle righe 2-6. Difatti, l'inizializzazione è una procedura molto semplice che imposta la distanza di ogni nodo dalla sorgente a $+\infty$ e poi azzera la distanza del nodo sorgente. Il codice nelle righe 7-11 consiste invece in un ciclo per verificare o meno la presenza di cicli di costo negativo.
	
	Il corpo dell'algoritmo esegue un'operazione di rilassamento (o meglio, invoca la procedura Relax) su ogni arco del grafo per $|V|-1$ iterazioni. Questo limite nasce dal numero di rilassamenti effettuati nel caso pessimo. Consideriamo un grafo in cui tutti i nodi sono collegati in linea, ognuno è collegato solo con il precedente ed il successivo tranne per il primo e l'ultimo. Se la sorgente è il primo nodo, Bellman-Ford dovrà effettuare esattamente $|V|-1$ iterazioni di rilassamento perchè coincide con il numero di archi nel cammino più lungo all'interno del grafo e ogni iterazione di rilassamento rilassa almeno un arco. Come si può notare, questo algoritmo effettua un'enorme quantità di calcoli inutili ed è proprio per questo che sono state proposte molte ottimizzazioni per ridurre il più possibile sia il numero complessivo di iterazioni di rilassamento che il numero di rilassamenti in ogni iterazione. Ad esempio, il limite massimo di iterazioni di rilassamento da effettuare corrisponde al diametro del grafo (molto inferiore al numero di nodi nel caso di grafi densi). Un'ulteriore ottimizzazione può essere il controllo sugli effettivi rilassamenti effettuati durante un'iterazione: se un'iterazione non ha prodotto alcun rilassamento, sicuramente non ne verranno prodotti altri e quindi l'algoritmo può terminare. Infine, è inefficiente effettuare un rilassamento su ogni arco del grafo al di fuori della prima iterazione. In tutte le successive iterazioni, è sufficiente effettuare un rilassamento solamente sugli archi adiacenti ad almeno un arco che è stato rilassato nell'iterazione precedente.
	
	La procedura Relax\ref{alg:relax} richiede in input due nodi ($u$ e $v$) e la funzione $w$ che restituisce, dati due nodi, il peso dell'arco tra i due. Questa procedura non consiste in un effettivo rilassamento dell'arco $(u,v)$ ma è un tentativo di rilassamento, infatti viene verificato se una riduzione della distanza del nodo $v$ dalla sorgente sia possibile. In parole povere, la procedura Relax verifica se "arrivare al nodo $v$ passando per $u$ come ultimo nodo" sia meno costoso del valore già presente.
	
	Questo algoritmo riesce a rilevare i cicli di costo negativo verificando se è possibile rilassare un arco alla fine delle $|V|-1$ iterazioni. Se in un grafo è presente un ciclo di costo negativo, almeno un arco di quel ciclo verrà rilassato ad ogni iterazione. Questo è dovuto alla presenza di un arco di costo negativo che permette sempre di ridurre la distanza dal nodo sorgente. Come già spiegato nella sezione \ref{section:sssp}, quando è presente un ciclo di costo negativo è sempre possibile ridurre il costo di un cammino che passa per esso, fino a $-\infty$. Per questo motivo, se alla fine delle $|V|-1$ iterazioni, almeno un arco è ancora rilassabile, allora vuol dire che è presente un ciclo di costo negativo e l'arco rilassabile ne fa parte.

	\begin{algorithm}[H]
		\label{alg:init}
		\SetAlgorithmName{Initialize-Single-Source}{}{}
		\KwIn{G, s}
		\ForEach{vertex v $\in$ G.V}{
			v.d = $\infty$\;
			v.$\pi$ = NIL\;
		}
		s.d = 0\;
		\caption{La procedura di inizializzazione del grafo}
	\end{algorithm}

	\begin{algorithm}[H]
		\label{alg:relax}
		\SetAlgorithmName{Relax}{}{}
		\KwIn{u, v, w}
		\If{v.d > u.d + w(u,v)}{
			v.d = u.d + w(u,v)\;
			v.$\pi$ = u\;
		}
		\caption{La procedura di rilassamento di un arco}
	\end{algorithm}

	\subsection{Costo computazionale}
	La procedura di inizializzazione\ref{alg:init} esegue due operazioni di costo $O(1)$ in ciascuna delle $|V|$ iterazioni e infine una singola istruzione da $O(1)$. Dunque, la procedura \texttt{Initialize-Single-Source} complessivamente ha un costo di $O(|V|)$. La procedura Relax\ref{alg:relax} esegue un confronto e, se verificato, esegue due istruzioni di costo $O(1)$. Complessivamente, dunque, la procedura Relax ha costo computazionale $O(1)$. Le istruzioni tra le righe 2-6 dell'algoritmo sono un doppio ciclo annidato che, per ognuna delle $|V|-1$ iterazioni, invoca la procedura Relax su ogni arco, dunque per $|E|$ volte. Quindi, questa porzione dell'algoritmo ha un costo computazionale di $O(|V|\cdot |E|)$. Le istruzioni sulle righe 7-11 complessivamente hanno un costo di $O(|E|)$ poichè esegue un confronto (di costo costante) per $|E|$ volte. Complessivamente, l'algoritmo ha costo computazionale $O(|V|\cdot |E|)$.
	
	\subsection{Dove parallelizzare}
	Analizziamo ora le dipendenze tra dati contenuti nell'algoritmo. Ognuna delle iterazioni del ciclo sulle righe 2-6 è dipendente dal risultato dell'iterazione precedente. Mentre ogni iterazione del ciclo tra le righe 3-5 presenta una dipendenza particolare: ogni rilassamento di un arco $(u,v)$ dipende dal risultato dell'ultimo rilassamento di un arco diretto verso $v$.
	
	Alternativamente, questo algoritmo può essere alterato scambiando l'ordine dei due cicli annidati. Questa variazione lo rende incorretto solamente se seriale. Se consideriamo una versione parallela di questa variante, si può interpretare il lavoro effettuato da un thread come un rilassamento seguito da massimo $|V|-1$ tentativi di aggiornamento concorrente del valore della distanza di $v$.
	
	\subsection{Precisazioni}
	Siccome, per lo scopo di questa tesi, non è necessario che l'algoritmo riesca a costruire l'albero dei cammini minimi, non è stata implementata questa funzionalità. Con riferimento allo pseudocodice, questa modifica si traduce nella rimozione della riga 3 dalla procedura Relax (\ref{alg:relax}) e dall'inizializzazione (\ref{alg:init}). Inoltre, i test utilizzati per le misurazioni non presentano archi di costo negativo e di conseguenza nessun ciclo di costo negativo. Per questo motivo, l'algoritmo implementato produce solamente un array di distanze di ogni nodo dalla sorgente.
	(quest'ultimo paragrafo andrebbe spostato dentro il design o dentro i dettagli implementativi)
	
	\section{Design}
	\label{section:design}
	Basandoci sull'analisi effettuata nel capitolo precedente, proponiamo di seguito tre implementazioni differenti dell'algoritmo di Bellman-Ford.
	
	\subsection{La versione \texttt{bf0}}
	La prima versione sfrutta pesantemente il parallelismo massivo offerto da CUDA, memorizzando il grafo di input come una lista di archi e parallelizzando il ciclo "interno" dell'algoritmo utilizzando un thread per ogni arco del grafo. Ad ogni iterazione, ogni thread esegue una procedura Relax su un singolo arco del grafo e poi si sincronizza con tutti gli altri.
	
	Il problema principale di questa versione consiste negli aggiornamenti concorrenti della distanza del nodo $v$. Come già accennato in precedenza, possiamo interpretare il lavoro svolto da ogni thread come $|V|-1$ tentativi di aggiornamento di un singolo valore. Per questo motivo, riteniamo possa essere comunque corretta una versione che non utilizza meccanismi di mutua esclusione. Tale versione sarà chiamata \texttt{bf0-none}. Ovviamente proponiamo anche la sua controparte \texttt{bf0-mutex} per sfruttare le operazioni atomiche offerte da CUDA.
	
	\subsection{Rimuovere le mutex: \texttt{bf1}}
	Il problema degli aggiornamenti concorrenti può essere risolto senza mutex ma utilizzando un numero molto inferiore di thread. Se il grafo di input viene memorizzato come lista di adiacenza in cui ogni nodo ha un riferimento solo ai nodi raggiungibili tramite archi uscenti, allora noi possiamo parallelizzare i rilassamenti da effettuare su ogni nodo. Nel dettaglio, fissato un nodo $u$, utilizziamo un thread per ogni arco uscente da $u$ che vi effettua un rilassamento. Com'è evidente, questa versione non presenta il problema di aggiornamenti concorrenti, ma utilizza molti meno thread. Le prestazioni di questa versione potrebbero essere comparabili a quella precedente solamente su grafi piccoli e completi, in cui ogni nodo ha un elevato numero di archi uscenti.
	
	\subsection{Sfruttare una reduction: \texttt{bf2}}
	\'E possibile migliorare la versione \texttt{bf1} senza aggiungere aggiornamenti concorrenti. In questo caso il grafo dovrebbe essere memorizzato come una lista di adiacenza in cui ogni nodo memorizza il riferimenti ai nodi che possono raggiungerlo, dunque gli archi entranti. Con questa modifica, il numero di thread utilizzabili aumenta perchè è possibile effettuare una min-reduction dei rilassamenti degli archi entranti su ogni nodo del grafo contemporaneamente.
	
	\begin{table}[!ht]
		\centering
		\begin{tabular}{|l|c|c|c|}
			\hline
			\textbf{Nome versione} & \textbf{Mutex} & \textbf{AoS/SoA} & \textbf{Shared Memory} \\ \hline
			\texttt{bf0-none-aos-nosh}  & no & AoS & no \\ \hline
			\texttt{bf0-none-aos-sh}    & no & AoS & si \\ \hline
			\texttt{bf0-none-soa-nosh}  & no & SoA & no \\ \hline
			\texttt{bf0-none-soa-sh}    & no & SoA & si \\ \hline
			\texttt{bf0-mutex-aos-nosh} & si & AoS & no \\ \hline
			\texttt{bf0-mutex-aos-sh}   & si & AoS & si \\ \hline
			\texttt{bf0-mutex-soa-nosh} & si & SoA & no \\ \hline
			\texttt{bf0-mutex-soa-sh}   & si & SoA & si \\ \hline
		\end{tabular}
		\label{tab:riassunto_bf0}
		\caption{Tabella riassuntiva delle caratteristiche delle sotto-versioni dell'algoritmo \texttt{bf0}}
	\end{table}

	\begin{table}[!ht]
		\centering
		\begin{tabular}{|l|c|c|}
			\hline
			\textbf{Nome versione} & \textbf{AoS/SoA} & \textbf{Shared Memory} \\ \hline
			\texttt{bf1-aos-nosh}  & AoS & no \\ \hline
			\texttt{bf1-aos-sh}    & AoS & si \\ \hline
			\texttt{bf1-soa-nosh}  & SoA & no \\ \hline
			\texttt{bf1-soa-sh}    & SoA & si \\ \hline
		\end{tabular}
		\label{tab:riassunto_bf1}
		\caption{Tabella riassuntiva delle caratteristiche delle sotto-versioni dell'algoritmo \texttt{bf1}}
	\end{table}

	\begin{table}[!ht]
		\centering
		\begin{tabular}{|l|c|c|}
			\hline
			\textbf{Nome versione} & \textbf{AoS/SoA} & \textbf{Shared Memory} \\ \hline
			\texttt{bf2-aos-nosh}  & AoS & no \\ \hline
			\texttt{bf2-aos-sh}    & AoS & si \\ \hline
			\texttt{bf2-soa-nosh}  & SoA & no \\ \hline
			\texttt{bf2-soa-sh}    & SoA & si \\ \hline
		\end{tabular}
		\label{tab:riassunto_bf2}
		\caption{Tabella riassuntiva delle caratteristiche delle sotto-versioni dell'algoritmo \texttt{bf2}}
	\end{table}
	
	\section{Dettagli implementativi}
	\label{section:impl}
	Serve fare un capitolo a parte per dei dettagli?
	
	Bisogna specificare che nei programmi della repo il nodo sorgente in ogni esecuzione dell'algoritmo è sempre il primo (quello di indice 0).
	
	O in questo capitolo o prima bisogna dire che, siccome per i nostri scopi non serve che l'algoritmo produca come risultato anche tutti i percorsi (nella forma di un vettore di predecessori) Bellman-Ford non è stato implementato in questo modo.
	
	Qui parlo del problema dell'overflow e della funzione atomicMin. Magari anche di come ho implementato le mutex nella versione \texttt{bf0-mutex}.
	
	Tutti gli algoritmi che utilizzano un buffer nella shared memory lo utilizzano per memorizzare i dati del "vettore \texttt{graph}" (invece del "vettore \texttt{distances}" oppure una soluzione ibrida) perchè si è rivelata la soluzione migliore e la più veloce a seguito di prove empiriche i cui risultati non sono riportati qui.
	
	La versione \texttt{bf0-none-aos-sh} usa un buffer shared per il vettore \texttt{graph} e non per il vettore \texttt{distances} perchè nel primo caso si ottiene un throughput fino a $1,121\cdot 10^9$ mentre nel secondo caso si raggiunge a fatica $1\cdot 10^9$ rilassamenti al secondo. Lo stesso discorso vale per tutte le altre versioni implementate, si utilizza la shared memory su graph (o sui vettori nel caso di \texttt{soa}) perchè produce un throughput maggiore rispetto a quando la si usa sul vettore \texttt{distances}. Qui andrebbe scritto qualcosa del tipo "nonostante in teoria un buffering del vettore \texttt{distances} riduce maggiormente il numero di accessi alla memoria globale, nella pratica gli accessi estremamenti sparsi su tutto il vettore producono un risultato molto più lento della versione senza shared memory.
	
	Aggiungere dettagli sui limiti delle varie versioni dell'algoritmo: ad esempio, le versioni \texttt{bf0}, siccome creano un thread per ogni arco, funzionano bene solo se il grafo ha meno di $2^{31}\cdot 1024$ archi (max numero di blocchi per max numero di thread). Le versioni \texttt{bf2} creano un blocco di thread per ogni nodo, quindi può gestire un grafo con massimo $2^{31}$ nodi.
	
	Le versioni \texttt{bf1} e \texttt{bf2} non funzionano bene su grafi con nodi isolati. In particolare, siccome l'algoritmo \texttt{bf1} memorizza il grafo come una lista di adiacenza degli archi uscenti, questi "non funziona bene" se è presente un nodo senza archi uscenti. Lo stesso discorso vale per \texttt{bf2} ma con gli archi entranti. Invece l'algoritmo \texttt{bf0} semplicemente produrrà un valore di $+\infty$ per tutti i nodi irraggiungibili.
	
	\section{Valutazione delle prestazioni}
	\label{section:perf}
	I test di correttezza di ogni versione vanno effettuati su ogni test disponibile. Ovviamente, solo le versioni che producono un risultato corretto in tutti i casi vengono tenute in considerazione nei capitoli successivi. I test per la valutazione delle prestazioni, invece, possono essere eseguiti solo sul test più voluminoso per ridurre i tempi di valutazione.
	
	\subsection{Dati della macchina su cui misuriamo l'algoritmo seriale}
	\begin{table}[!ht]
		\centering
		\begin{tabular}{|l|c|c|c|}
			\hline
			& \textbf{Macchina A} & \textbf{Macchina B} & \textbf{Macchina C} \\ \hline
			\textbf{Sistema Operativo} & Windows & Ubuntu & Windows \\ \hline
			\textbf{Versione SO} & 10.0.19042.1165 & 16.04.7 & 10.0.19043.1165 \\ \hline
			\textbf{Versione Compilatore C} & CL 19.29.30133 & GCC 5.4.0 & CL 19.29.30040 \\ \hline
			\textbf{Nome CPU} & Intel Core i5-7200U & Intel Xeon E5-2603 v4 & Intel Core i7-8700 \\ \hline
			\textbf{Frequenza CPU} & 2,5 - 2,7 GHz & 1,2 - 1,7 GHz & 3,2 GHz \\ \hline
			\textbf{Cores fisici} & 2 & 6 & 6 \\ \hline
			\textbf{Hyper-Threading} & si & no & si \\ \hline
			\textbf{L1 cache} & 128 KB & 32 KB & 384 KB \\ \hline
			\textbf{L2 cache} & 512 KB & 256 KB & 1,5 MB \\ \hline
			\textbf{L3 cache} & 3 MB & 15 MB & 12 MB \\ \hline
			\textbf{RAM totale} & 8 GB & 64 GB & 32 GB \\ \hline
			\textbf{Frequenza RAM} & 2133 MHz & 2400 MHz & 2133 MHz \\ \hline
			\textbf{Tipologia RAM} & DDR4 & DDR4 & DDR4 \\ \hline
			\textbf{Nome GPU} & \textbf{GeForce 940MX} & \textbf{GeForce GTX 1070} & \textbf{GeForce RTX 2080} \\ \hline
			\textbf{Versione CUDA Driver} & 11.4 & 10.1 & 11.4 \\ \hline
			\textbf{Versione CUDA Runtime} & 11.1 & 8.0 & 11.1 \\ \hline
			\textbf{CUDA compute capability} & 5.0 & 6.1 & 7.5 \\ \hline
			\textbf{Architettura GPU} & Maxwell & Pascal & Turing \\ \hline
			\textbf{Memoria globale GPU} & 2 GB & 8 GB & 8 GB \\ \hline
			\textbf{Tipologia Memoria GPU} & GDDR5 & GDDR5 & GDDR6 \\ \hline
			\textbf{Frequenza Memoria} & 2505 MHz & 4004 MHz & 7000 MHz \\ \hline
			\textbf{Ampiezza Bus GPU} & 64 bit & 256 bit & 256 bit \\ \hline
			\textbf{Streaming Multiprocessors} & 3 & 15 & 46 \\ \hline
			\textbf{CUDA cores totali} & 384 & 1920 & 2944 \\ \hline
			\textbf{Frequenza Cores} & 1189 MHz & 1797 MHz & 1860 MHz \\ \hline
		\end{tabular}
		\label{tab:specs}
		\caption{Specifiche delle macchine su cui effettuiamo le misurazioni}
	\end{table}

	\subsection{Performance dell'algoritmo seriale}
	\begin{table}[!ht]
		\centering
		\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Nome test} & \textbf{Wall Clock Time [s]} & \textbf{Throughput [$10^6$ relax/s]\ref{foot:relax}} \\ \hline
			       rome & 0,38 & 78 \\ \hline
			        DE & 81,45 & 75 \\ \hline
			        VT & 217,06 & 95,8 \\ \hline
			        ME & 874,61 & 94 \\ \hline
			        NV & 1743,02 & 92 \\ \hline
			        025 & 2,39 & 104,2 \\ \hline
			        050 & 4,55 & 109,4 \\ \hline
			        075 & 6,8 & 109,8 \\ \hline
			        100 & 8,12 & 122,4 \\ \hline
		\end{tabular}
		\label{tab:performance_serial}
		\caption{Prestazioni dell'algoritmo seriale su CPU (sulla "macchina A")}
	\end{table}
	\footnote{\label{foot:relax} numero di rilassamenti effettuati al secondo}

	\subsection{Descrizione dei test}
	Qui parlo di come sono strutturati i file di test e cosa rappresentano.
	
	Tutti i valori riportati nelle tabelle sono ottenuti misurandoli sul test più voluminoso (NV). Le tabelle complete saranno inserite come file Excel a parte dentro la repo.
	\begin{table}[!ht]
		\centering
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{Nome test} & \textbf{Numero nodi} & \textbf{Numero archi} & \textbf{Descrizione} \\ \hline
			rome & 3353 & 8859 & Mappa metropolitana di Roma \\ \hline
			DE & 49109 & 119744 & Mappa stradale del Delaware \\ \hline
			VT & 97975 & 212979 & Mappa stradale del Vermont \\ \hline
			ME & 194505 & 425708 & Mappa stradale del Maine \\ \hline
			NV & 261155 & 618175 & Mappa stradale del Nevada \\ \hline
			025 & 1000 & 249808 & Grafo casuale con densità 0.25 \\ \hline
			050 & 1000 & 499574 & Grafo casuale con densità 0.5 \\ \hline
			075 & 1000 & 749262 & Grafo casuale con densità 0.75 \\ \hline
			100 & 1000 & 999000 & Grafo casuale completo \\ \hline
		\end{tabular}
		\label{tab:riassunto_test}
		\caption{Tabella riassuntiva delle specifiche dei test utilizzati}
	\end{table}

	\subsection{Correttezza degli algoritmi}
	Qui dobbiamo scartare le eventuali versioni che non producono un risultato corretto su tutti i test disponibili.
	
	Tutti gli algoritmi producono il risultato corretto sul grafo di Roma, tranne \texttt{bf2-soa-sh}.
	
	\subsection{Speedup}
	Di seguito riportiamo i valori di speedup di ogni versione, rispetto al tempo d'esecuzione dell'algoritmo \texttt{bf-serial}, misurati su ogni scheda.
	\begin{table}[!ht]
		\centering
		\begin{tabular}{|l|c|c|c|}
			\hline
			\textbf{Nome algoritmo} & \textbf{940MX} & \textbf{GTX 1070} & \textbf{RTX 2080} \\ \hline
			\texttt{bf0-none-aos-nosh}  &  &  &  \\ \hline
			\texttt{bf0-none-aos-sh}    &  &  &  \\ \hline
			\texttt{bf0-none-soa-nosh}  &  &  &  \\ \hline
			\texttt{bf0-none-soa-sh}    &  &  &  \\ \hline
			\texttt{bf0-mutex-aos-nosh} &  &  &  \\ \hline
			\texttt{bf0-mutex-aos-sh}   &  &  &  \\ \hline
			\texttt{bf0-mutex-soa-nosh} &  &  &  \\ \hline
			\texttt{bf0-mutex-soa-sh}   &  &  &  \\ \hline
		\end{tabular}
		\label{tab:speedup_bf0}
		\caption{Misurazioni dello speedup per le versioni dell'algoritmo \texttt{bf0}}
	\end{table}

	\begin{table}[!ht]
		\centering
		\begin{tabular}{|l|c|c|c|}
			\hline
			\textbf{Nome algoritmo} & \textbf{940MX} & \textbf{GTX 1070} & \textbf{RTX 2080} \\ \hline
			\texttt{bf1-aos-nosh}  &  &  &  \\ \hline
			\texttt{bf1-aos-sh}    &  &  &  \\ \hline
			\texttt{bf1-soa-nosh}  &  &  &  \\ \hline
			\texttt{bf1-soa-sh}    &  &  &  \\ \hline
		\end{tabular}
		\label{tab:speedup_bf1}
		\caption{Misurazioni dello speedup per le versioni dell'algoritmo \texttt{bf1}}
	\end{table}

	\begin{table}[!ht]
		\centering
		\begin{tabular}{|l|c|c|c|}
			\hline
			\textbf{Nome algoritmo} & \textbf{940MX} & \textbf{GTX 1070} & \textbf{RTX 2080} \\ \hline
			\texttt{bf2-aos-nosh}  &  &  &  \\ \hline
			\texttt{bf2-aos-sh}    &  &  &  \\ \hline
			\texttt{bf2-soa-nosh}  &  &  &  \\ \hline
		\end{tabular}
		\label{tab:speedup_bf2}
		\caption{Misurazioni dello speedup per le versioni dell'algoritmo \texttt{bf2}}
	\end{table}
	
	\subsection{Throughput}
	Di seguito riportiamo i valori di throughput di ogni versione, indicati come numero di rilassamenti al secondo, misurati su ogni scheda.
	\begin{table}[!ht]
		\centering
		\begin{tabular}{|l|c|c|c|}
			\hline
			\textbf{Nome versione} & \textbf{940MX} & \textbf{GTX 1070} & \textbf{RTX 2080} \\ \hline
			\texttt{bf0-none-aos-nosh}  &  &  &  \\ \hline
			\texttt{bf0-none-aos-sh}    &  &  &  \\ \hline
			\texttt{bf0-none-soa-nosh}  &  &  &  \\ \hline
			\texttt{bf0-none-soa-sh}    &  &  &  \\ \hline
			\texttt{bf0-mutex-aos-nosh} &  &  &  \\ \hline
			\texttt{bf0-mutex-aos-sh}   &  &  &  \\ \hline
			\texttt{bf0-mutex-soa-nosh} &  &  &  \\ \hline
			\texttt{bf0-mutex-soa-sh}   &  &  &  \\ \hline
		\end{tabular}
		\label{tab:throughput_bf0}
		\caption{Misurazioni del throughput per le versioni dell'algoritmo \texttt{bf0}}
	\end{table}

	\begin{table}[!ht]
		\centering
		\begin{tabular}{|l|c|c|c|}
			\hline
			\textbf{Nome algoritmo} & \textbf{940MX} & \textbf{GTX 1070} & \textbf{RTX 2080} \\ \hline
			\texttt{bf1-aos-nosh}  &  &  &  \\ \hline
			\texttt{bf1-aos-sh}    &  &  &  \\ \hline
			\texttt{bf1-soa-nosh}  &  &  &  \\ \hline
			\texttt{bf1-soa-sh}    &  &  &  \\ \hline
		\end{tabular}
		\label{tab:throughput_bf1}
		\caption{Misurazioni del throughput per le versioni dell'algoritmo \texttt{bf1}}
	\end{table}
	
	\begin{table}[!ht]
		\centering
		\begin{tabular}{|l|c|c|c|}
			\hline
			\textbf{Nome algoritmo} & \textbf{940MX} & \textbf{GTX 1070} & \textbf{RTX 2080} \\ \hline
			\texttt{bf2-aos-nosh}  &  &  &  \\ \hline
			\texttt{bf2-aos-sh}    &  &  &  \\ \hline
			\texttt{bf2-soa-nosh}  &  &  &  \\ \hline
		\end{tabular}
		\label{tab:throughput_bf2}
		\caption{Misurazioni del throughput per le versioni dell'algoritmo \texttt{bf2}}
	\end{table}
	
	
	
	Fonte dei test:
	\begin{itemize}
		\item \url{https://www.moreno.marzolla.name/teaching/LabASD/handouts/bellman-ford.html}
		\item \url{http://users.diag.uniroma1.it/challenge9/download.shtml}
	\end{itemize}
	
	\section{Conclusioni}
	\label{section:end}
	Qui dovrei effettuare un'analisi di tutti i risultati ottenuti e scegliere infine la versione migliore da etichettare come "Bellman-Ford in CUDA".
	
	\printbibliography
	
\end{document}